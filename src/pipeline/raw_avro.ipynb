{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "[X] Implement a delta function checker - if it's X number of days behind current day just stop reading\n",
    "\n",
    "[X] Currently overwrites data that already exists. This needs to stop\n",
    "\n",
    "[ ] Store the files and put them into folders so you don't rip your eyes out when opening the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571266832.231919\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import dask.bag as db\n",
    "import json\n",
    "from zefr.mlpl.util import feature_importance_plot, train_validation_plot, glob_s3\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time() \n",
    "print(start_time)\n",
    "\n",
    "# # Dask Client Parameters\n",
    "CLIENT_INIT_TIMEOUT = 1200\n",
    "\n",
    "# # Pipeline parameters\n",
    "DASK_SCHEDULER = \"tcp://dask.zefr.com:8798\"  # Ian's scheduler\n",
    "STORAGE_PATH = \"../../avro_deltas/\"\n",
    "TRAINING_PATH = \"s3://zefr/kafka/prod/topics/qz-qzapi-review\"\n",
    "SEEN_FILES = 'seen_files.txt'  # So we don't re-write files we've seen. Addresses point 2 above\n",
    "DATA_SRC = glob_s3(TRAINING_PATH)\n",
    "\n",
    "DAYS_DIFFERENCE = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "#  This cell addresses issue DSMP-241.  When the dask cluster is started and \n",
    "#  the notebook is run, there's a race condition between the workers being\n",
    "#  available and the use of the workers.  If an attempt to use the workers is\n",
    "#  is initiated before the workers are available, a TimeoutError indicating\n",
    "#  no workers are available will be raised by Dask. \n",
    "#\n",
    "#  This code creates the client and if the desired number of workers are\n",
    "#  not detected, it will restart the client (with exponential backoff) until \n",
    "#  the workers come up or a timeout occurs.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "from typing import Callable, Optional, TypeVar, Union\n",
    "import time\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "S = TypeVar(\"S\")\n",
    "\n",
    "def exp_backoff(\n",
    "    init_data: T,\n",
    "    init_fn: Callable[[T], S],\n",
    "    state_transition_fn: Callable[[S, T], S],\n",
    "    acceptance_criteria: Callable[[S], bool],\n",
    "    init_delay_sec: float = 1,\n",
    "    delay_exponent: float = 2,\n",
    "    timeout_sec: float = 30,\n",
    "    logger = None,\n",
    "    no_sleep_til_brooklyn: bool = False\n",
    ") -> Union[TimeoutError, S]:\n",
    "\n",
    "    assert init_delay_sec >= 0, f\"init_delay_sec must be non-negative, found: {init_delay_sec}\"\n",
    "    assert delay_exponent >= 1, f\"delay_exponent must be >= 1, found: {delay_exponent}\"\n",
    "\n",
    "    retries = 0\n",
    "    running_time = 0\n",
    "    timeout = init_delay_sec\n",
    "    s = init_fn(init_data)\n",
    "    accept = acceptance_criteria(s)\n",
    "\n",
    "    while not accept and running_time * delay_exponent < timeout_sec:\n",
    "        print(f\"Retrying. sleeping {timeout} seconds\")\n",
    "        if no_sleep_til_brooklyn is False:\n",
    "            if logger is not None:\n",
    "                logger.info(f\"Retrying. sleeping {timeout} seconds\")\n",
    "            time.sleep(timeout)\n",
    "        running_time += timeout\n",
    "        timeout *= delay_exponent\n",
    "        retries += 1\n",
    "        s = state_transition_fn(s, init_data)\n",
    "        accept = acceptance_criteria(s)\n",
    "\n",
    "    if accept:\n",
    "        return s\n",
    "    else:\n",
    "        if logger is not None:\n",
    "            logger.error(\n",
    "                \"Retries unsuccessful. \"\n",
    "                f\"retries: {retries}, \"\n",
    "                f\"agg_delay_time: {running_time}, \"\n",
    "                f\"timeout_sec: {timeout_sec}, \"\n",
    "                f\"init_delay_sec: {init_delay_sec}, \"\n",
    "                f\"delay_exponent: {delay_exponent}\"\n",
    "            )\n",
    "        return TimeoutError(\n",
    "            \"Retries unsuccessful. \"\n",
    "            f\"retries: {retries}, \"\n",
    "            f\"agg_delay_time: {running_time}, \"\n",
    "            f\"timeout_sec: {timeout_sec}, \"\n",
    "            f\"init_delay_sec: {init_delay_sec}, \"\n",
    "            f\"delay_exponent: {delay_exponent}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dask_client(\n",
    "    scheduler_uri: Optional[str] = None, \n",
    "    min_workers = 0,\n",
    "    init_delay_sec: float = 1,\n",
    "    delay_exponent: float = 2,\n",
    "    timeout_sec: float = 30,\n",
    "    logger = None\n",
    ") -> Client:\n",
    "\n",
    "    err_or_client = exp_backoff(\n",
    "      init_data = scheduler_uri, \n",
    "      init_fn = lambda scheduler_uri: Client(scheduler_uri),\n",
    "      state_transition_fn = lambda client, _: client.restart(),\n",
    "      acceptance_criteria = lambda client: min_workers <= len(client.get_worker_logs()),\n",
    "      init_delay_sec = init_delay_sec,\n",
    "      delay_exponent = delay_exponent,\n",
    "      timeout_sec = timeout_sec,\n",
    "      logger = logger\n",
    "    )\n",
    "\n",
    "    if isinstance(err_or_client, TimeoutError):\n",
    "        raise err_or_client\n",
    "    else: \n",
    "        return err_or_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://dask.zefr.com:8798\n",
       "  <li><b>Dashboard: </b><a href='http://dask.zefr.com:8799/status' target='_blank'>http://dask.zefr.com:8799/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://172.31.27.0:8798' processes=0 cores=0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = get_dask_client(DASK_SCHEDULER, min_workers=0, timeout_sec=CLIENT_INIT_TIMEOUT)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_none_records(video_message):\n",
    "    return video_message['metadata'] is not None and video_message['payload'] is not None\n",
    "\n",
    "\n",
    "def generate_f_name(s3_f_name):\n",
    "    \"\"\"\n",
    "    Given our glob_s3, spits out the avro deltas of our data\n",
    "    \n",
    "    # s3://zefr/kafka/prod/topics/qz-qzapi-review/year=2019/month=10/day=14/qz-qzapi-review+37+0000037164.avro\n",
    "    \"\"\"\n",
    "    \n",
    "    name_split = s3_f_name.split('/')\n",
    "    year = name_split[-4][-4:]\n",
    "    month = name_split[-3][-2:]\n",
    "    day = name_split[-2][-2:]\n",
    "    qz_id = name_split[-1]  # 'qz-qzapi-review+37+0000037164.avro'\n",
    "    \n",
    "    try:\n",
    "        year = int(year)  # 'year=2019'\n",
    "        month = int(month)  # 'month=10'\n",
    "        day = int(day)  # 'day=14'\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print('Failed on: {}'.format(s3_f_name))\n",
    "        print('Tried to convert a non-integer char into an integer: {}'.format(e))\n",
    "        print('year: {}'.format(year))\n",
    "        print('month: {}'.format(month))\n",
    "        print('day: {}'.format(day))\n",
    "        print('*' * 10)\n",
    "        return False  # \n",
    "    \n",
    "    \n",
    "    output_name = '{}_{}_{}_{}'.format(year, month, day, qz_id)\n",
    "    return output_name\n",
    "\n",
    "\n",
    "def generate_date(s3_f_name):\n",
    "    name_split = s3_f_name.split('/')\n",
    "    year = name_split[-4][-4:]\n",
    "    month = name_split[-3][-2:]\n",
    "    day = name_split[-2][-2:]\n",
    "    qz_id = name_split[-1]  # 'qz-qzapi-review+37+0000037164.avro'\n",
    "    \n",
    "    return datetime.strptime(\"{}/{}/{}\".format(day, month, year), \"%d/%m/%Y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}{}'.format(STORAGE_PATH, SEEN_FILES), 'r') as f:\n",
    "    previously_seen = set([f_name.strip() for f_name in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9307dcc13a4569adf5add09439f360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4525), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seen_files = set()\n",
    "todays_date = datetime.today()\n",
    "\n",
    "# We go from most recent to most distant date\n",
    "for f_name in tqdm(DATA_SRC[::-1]):\n",
    "    \n",
    "    get_date = generate_date(f_name)\n",
    "    \n",
    "    if abs(todays_date - get_date) > DAYS_DIFFERENCE:\n",
    "        break  # Gone beyond #days we want to consider, and since sorted recent -> distant, break\n",
    "    \n",
    "    # Already have the data, no sense in downloading it again\n",
    "    if f_name in previously_seen:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Read the deltas\n",
    "    source = db.read_avro(f_name)\n",
    "    \n",
    "    # Extract the data with meaningful information\n",
    "    resp = source.filter(lambda video_message: filter_none_records(video_message)).compute()\n",
    "    \n",
    "    name = generate_f_name(f_name)\n",
    "    if not name:  # There was an error generating the name\n",
    "        continue\n",
    "    \n",
    "    with open('{}{}'.format(STORAGE_PATH, name), 'w') as f:\n",
    "        json.dump(resp, f)\n",
    "        \n",
    "    seen_files.add(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}{}'.format(STORAGE_PATH, SEEN_FILES), 'w') as f:\n",
    "    for sf in seen_files:\n",
    "        f.write(\"{}\\n\".format(sf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
